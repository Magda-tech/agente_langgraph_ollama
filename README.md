# ü§ñ Magda Assistente com LangGraph + Ollama

Este projeto implementa um agente inteligente local utilizando a biblioteca [LangGraph](https://python.langchain.com/docs/langgraph/) com suporte a modelos LLM via [Ollama](https://ollama.com/), permitindo intera√ß√µes personalizadas, incluindo c√°lculo, busca simulada e respostas com LLMs locais. Com suporte a **mem√≥ria de contexto**, o assistente lembra das mensagens anteriores dentro da sess√£o.

## üöÄ Vis√£o geral

O agente √© composto por:

- Um **grafo de decis√£o** constru√≠do com `StateGraph` do LangGraph
- Um modelo local rodando com o **Ollama** (neste caso, `gemma3`)
- Tr√™s fun√ß√µes principais:
  - `calcular`: resolve perguntas matem√°ticas simples
  - `buscar`: simula uma busca local
  - `decidir`: atua como um roteador inteligente entre as op√ß√µes acima ou aciona o modelo LLM para perguntas abertas
- Suporte a **mem√≥ria** com `MemorySaver`, para manter o contexto entre as intera√ß√µes

---

## üõ†Ô∏è Como usar

1. Clone o reposit√≥rio para sua m√°quina:

```bash
git clone https://github.com/Magda-tech/agente_langgraph_ollama.git
```

2. Crie e ative o ambiente Conda:

```bash
conda create -n langgraph_ollama python=3.10
conda activate langgraph_ollama
cd agente_langgraph_ollama
pip install -r requirements.txt
```

3. Instale o pacote para conectar com o Ollama:

```bash
pip install langchain-ollama
```

4. Em outro terminal, inicie o modelo local:

```bash
ollama run gemma3
```

5. No VS Code:

   - Pressione `Ctrl+Shift+P`
   - Escolha `Python: Select Interpreter`
   - Selecione o ambiente `langgraph_ollama`

6. Execute o arquivo `agente_langgraph_ollama.py` diretamente no terminal ou pelo bot√£o de execu√ß√£o do VS Code.

Agora voc√™ pode digitar perguntas diretamente no terminal. Para sair, digite `sair`.

---

## üß† Como funciona o c√≥digo

### 1. Esquema de estado com `TypedDict`

Define o formato do estado compartilhado entre os n√≥s:

```python
class AgentState(TypedDict):
    mensagens: List[BaseMessage]
```

> Usamos `mensagens` ao inv√©s de `mensagem/resposta`, para possibilitar a **mem√≥ria conversacional**.

### 2. Modelo local com `OllamaLLM`

Carrega o modelo `gemma3` via Ollama:

```python
llm = OllamaLLM(model="gemma3")
```

### 3. Fun√ß√µes do agente

- `calcular`: usa `eval()` para resolver perguntas matem√°ticas como "quanto √© 10 * 5"
- `buscar`: retorna uma resposta gen√©rica simulando uma base de dados
- `decidir`: roteador inteligente que identifica qual n√≥ seguir:
  - Se a pergunta cont√©m "quanto √©", envia para `calcular`
  - Se cont√©m "onde" ou "quando", envia para `buscar`
  - Caso contr√°rio, usa o LLM local para gerar uma resposta

### 4. Grafo com LangGraph

- O grafo √© criado com `StateGraph`
- Os n√≥s s√£o adicionados com `add_node()` usando `RunnableLambda`
- A l√≥gica de rota √© definida em `add_conditional_edges`

```python
lambda x: x["fluxo"] if x["fluxo"] in ["calculo", "busca"] else "fim"
```

### 5. Mem√≥ria com `MemorySaver`

```python
from langgraph.checkpoint.memory import MemorySaver

memory = MemorySaver()
app = graph.compile(checkpointer=memory)
```

Com isso, o agente consegue lembrar intera√ß√µes anteriores dentro de uma mesma sess√£o.

---

## üî™ Exemplos para testar

- `Quanto √© 8 * 5?`
- `Multiplique isso por 2` (ap√≥s um c√°lculo anterior)
- `Onde fica o Brasil?`

---

## üìÇ Arquivos principais

- `agente_langgraph_ollama.py`: c√≥digo principal do agente
- `requirements.txt`: depend√™ncias Python
- `.gitignore`: arquivos ignorados pelo Git

---

## üí° Sobre o decisor inteligente

O n√≥ `decidir` atua como um roteador autom√°tico:

- Analisa a mensagem de entrada
- Direciona o fluxo do grafo para o n√≥ adequado (`calculo`, `busca` ou `LLM`)
- Garante uma resposta contextual e adaptativa

Esse mecanismo permite que o agente combine l√≥gica determin√≠stica com a intelig√™ncia de um modelo local, criando um sistema responsivo e vers√°til.

---

## üîÅ Executando ap√≥s reiniciar o computador

Sempre que desligar ou reiniciar seu computador, **n√£o ser√° necess√°rio reinstalar nada**, apenas seguir os passos abaixo para reativar seu assistente:

### 1. Abra dois terminais

#### Terminal 1: Ative o ambiente Conda e rode o agente

```bash
conda activate langgraph_ollama
cd agente_langgraph_ollama
python agente_langgraph_ollama.py
```

#### Terminal 2: Inicie o modelo local via Ollama

```bash
ollama run gemma3
```

### ‚úÖ Pronto!

Agora voc√™ pode interagir normalmente com o assistente.\
Digite sua pergunta no primeiro terminal. Para encerrar, digite `sair`.

---

‚úÖ Feito por **Magda Monteiro** para aprender sobre agentes inteligentes com modelos locais.
